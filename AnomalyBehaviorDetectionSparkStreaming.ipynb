{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Spark Streaming-Behavior-Anomaly-Detection \n",
    "We will use Streaming data to detect anomlay in real time .\n",
    "\n",
    "We are ready to build t Machine learning program. we will proceed as follow:\n",
    "\n",
    "\n",
    "Step 1) Basic operation with PySpark\n",
    "\n",
    "Step 2) Data preprocessing\n",
    "\n",
    "Step 3) Build a data processing pipeline\n",
    "\n",
    "Step 4) Build the classifier\n",
    "\n",
    "Step 5) Train and evaluate the model\n",
    "\n",
    "Step 6) Tune the hyperparameter\n",
    "\n",
    "# Importing Packages and configuring spark engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()#create spark session \n",
    "sc = spark.sparkContext#create sparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "from operator import add\n",
    "from operator import add\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_pickle(\"C:/Users/rzouga/Downloads/Work Swiss/location-exploration/Recruitment-Challenge/locations.pkl\")\n",
    "dfspark = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerLocationState: string (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create new features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+---------+------------+------------+\n",
      "|customerLocationState|date      |hourStart|cumSumByHour|cumSumBydate|\n",
      "+---------------------+----------+---------+------------+------------+\n",
      "|outside              |2018-06-26|8        |1           |1           |\n",
      "|outside              |2018-07-09|8        |1           |1           |\n",
      "|unknown              |2018-07-16|11       |1           |1           |\n",
      "|unknown              |2018-07-16|15       |1           |2           |\n",
      "|bedroom              |2018-07-21|3        |1           |1           |\n",
      "|bedroom              |2018-07-21|3        |2           |2           |\n",
      "|bedroom              |2018-07-21|7        |1           |3           |\n",
      "|bedroom              |2018-07-21|10       |1           |4           |\n",
      "|bedroom              |2018-07-21|12       |1           |5           |\n",
      "|bedroom              |2018-07-21|15       |1           |6           |\n",
      "+---------------------+----------+---------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "dfspark =dfspark.withColumn(\"hourstart\", F.hour(F.col(\"startTime\")))\n",
    "dfspark=dfspark.withColumn(\"date\", F.to_date(F.col(\"startTime\")))\n",
    "\n",
    "\n",
    "\n",
    "cumSumByHour = Window.partitionBy(['date','customerLocationState','hourStart']).orderBy('startTime').rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "dfspark = dfspark.withColumn(\"cumSumByHour\",F.count('customerLocationState').over(cumSumByHour))\n",
    "cumSumBydate= Window.partitionBy(['date','customerLocationState']).orderBy('startTime').rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "dfspark = dfspark.withColumn(\"cumSumBydate\",F.count('customerLocationState').over(cumSumBydate))\n",
    "\n",
    "dfspark.select('customerLocationState',\"date\",'hourStart','cumSumByHour',\n",
    "                               'cumSumBydate').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerLocationState: string (nullable = true)\n",
      " |-- hourStart: integer (nullable = true)\n",
      " |-- cumSumByHour: long (nullable = false)\n",
      " |-- cumSumBydate: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsparkML=dfspark.select('customerLocationState','hourStart','cumSumByHour',\n",
    "                               'cumSumBydate')\n",
    "dfsparkML.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "## Feature Engineering in pyspark\n",
    "\n",
    "The most commonly used data pre-processing techniques in approaches in Spark are as follows\n",
    "\n",
    "1) VectorAssembler\n",
    "\n",
    "2)Bucketing\n",
    "\n",
    "3)Scaling and normalization\n",
    "\n",
    "4) Working with categorical features\n",
    "\n",
    "5) Text data transformers\n",
    "\n",
    "6) Feature Manipulation\n",
    "\n",
    "7) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data to dense vector (features):\n",
    "\n",
    "Note: You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[4,7,8,9],[1....|\n",
      "|(10,[4,7,8,9],[1....|\n",
      "|(10,[5,7,8,9],[1....|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_dummy(df,categoricalCols,continuousCols):\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "     # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+ continuousCols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    #data = data.withColumn('label',col(labelCol))\n",
    "    return data.select('features')\n",
    "\n",
    "catcols = ['customerLocationState']\n",
    "num_cols = ['hourStart','cumSumByHour','cumSumBydate']\n",
    "#labelCol ='label'\n",
    "data = get_dummy(dfsparkML,catcols,num_cols)\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nubmbers of rows\n",
    "n = data.count()\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing :\n",
    "**Note:** Since clustering algorithms including k-means use distance-based measurements to determine the\n",
    "similarity between data points, Itâ€™s strongly recommended to standardize the data to have a mean of zero\n",
    "and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "# Let us create an object of MinMaxScaler class\n",
    "MinMaxScaler=MinMaxScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "scalerModel= MinMaxScaler.fit(data)\n",
    "scaledData = scalerModel.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose the best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\n\\ncost = np.zeros(10)\\nfor k in range(2,10):\\n    kmeans = KMeans()              .setK(k)              .setSeed(1)               .setFeaturesCol(\"Scaled_features\")              .setPredictionCol(\"cluster\")\\n    model = kmeans.fit(scaledData)\\n    cost[k] = model.computeCost(scaledData) # requires Spark 2.0 or later'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "cost = np.zeros(10)\n",
    "for k in range(2,10):\n",
    "    kmeans = KMeans()\\\n",
    "              .setK(k)\\\n",
    "              .setSeed(1) \\\n",
    "              .setFeaturesCol(\"Scaled_features\")\\\n",
    "              .setPredictionCol(\"cluster\")\n",
    "    model = kmeans.fit(scaledData)\n",
    "    cost[k] = model.computeCost(scaledData) # requires Spark 2.0 or later\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"import numpy as np\\nimport matplotlib.mlab as mlab\\nimport matplotlib.pyplot as plt\\nimport seaborn as sbs\\nfrom matplotlib.ticker import MaxNLocator\\nfig, ax = plt.subplots(1,1, figsize =(8,6))\\nax.plot(range(2,10),cost[2:10])\\nax.set_xlabel(\\'k\\')\\nax.set_ylabel(\\'cost\\')\\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\\nplt.show()'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),cost[2:10])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Based on  the cost , we chose K=10\n",
    "###  Train the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    featuresCol=\"Scaled_features\", \n",
    "    predictionCol=\"cluster\", \n",
    "    k=10, \n",
    "    seed=0)\n",
    "\n",
    "model = kmeans.fit(scaledData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.53799194, 0.10801065, 0.23640452])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids=model.clusterCenters()\n",
    "centroids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0| 7043|\n",
      "|      1| 5064|\n",
      "|      2| 4395|\n",
      "|      3| 5272|\n",
      "|      4| 7111|\n",
      "|      5| 2118|\n",
      "|      6|  625|\n",
      "|      7|  424|\n",
      "|      8| 1858|\n",
      "|      9| 4954|\n",
      "+-------+-----+\n",
      "\n",
      "+--------------------+-------+\n",
      "|     Scaled_features|cluster|\n",
      "+--------------------+-------+\n",
      "|[0.0,0.0,0.0,0.0,...|      6|\n",
      "|[0.0,0.0,0.0,0.0,...|      6|\n",
      "|[0.0,0.0,0.0,0.0,...|      7|\n",
      "|[0.0,0.0,0.0,0.0,...|      7|\n",
      "|[0.0,0.0,0.0,1.0,...|      5|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(scaledData)\n",
    "\n",
    "prediction.groupBy(\"cluster\").\\\n",
    "\tcount().\\\n",
    "    orderBy(\"cluster\").\\\n",
    "    show()\n",
    "prediction.select(\"Scaled_features\", \n",
    "                  \"cluster\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold has strong impact on the result. Be careful when choosing it! A simple way to choose the threshold's value is picking up a distance of a data point from among known data. For example, the 100th-farthest data point distance can be an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the distance between each point and the centroid of the cluster it belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def get_distance_map(record,culster):\n",
    "    #cluster = clusters.predict(record)\n",
    "    centroid = model.clusterCenters()[cluster]\n",
    "    dist = np.linalg.norm(record - centroid)\n",
    "    return (dist)\n",
    "\n",
    "\n",
    "prediction =prediction.select(\"Scaled_features\",\"cluster\")\n",
    "def to_distances(entry):\n",
    "    data_point = entry[0]\n",
    "    prediction = entry[1]\n",
    "    centroid = model.centers[prediction]\n",
    "    return np.linalg.norm(data_point - centroid)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of centroids for every cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.53799194, 0.10801065, 0.23640452]),\n",
       " array([0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.79982313, 0.05952381, 0.22949373]),\n",
       " array([0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.56512836, 0.02203803, 0.07130262]),\n",
       " array([0.00000000e+00, 9.99810319e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.89681335e-04, 3.98132876e-01,\n",
       "        5.15662259e-02, 7.89686868e-02]),\n",
       " array([1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.8387323 , 0.10297093, 0.45606624]),\n",
       " array([0.        , 0.        , 0.        , 0.99952786, 0.        ,\n",
       "        0.        , 0.        , 0.31861477, 0.01794145, 0.02942536]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.9984    ,\n",
       "        0.        , 0.        , 0.58525217, 0.00685714, 0.00726667]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99292453, 0.00471698, 0.67124692, 0.01168014, 0.0226022 ]),\n",
       " array([0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.83153929, 0.02311754, 0.0872522 ]),\n",
       " array([9.99796417e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.03583062e-04, 2.72810154e-01,\n",
       "        6.70176051e-02, 6.88407641e-02])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clusters = model.clusterCenters()\n",
    "\n",
    "traind_clusters = {int(i):[float(train_clusters[i][j]) for j in range(len(train_clusters[i]))] \n",
    "              for i in range(len(train_clusters))}\n",
    "train_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.56512835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.9998103186646434, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                             center\n",
       "0        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...\n",
       "1        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...\n",
       "2        2  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.56512835...\n",
       "3        3  [0.0, 0.9998103186646434, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4        4  [0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_centers = spark.sparkContext.parallelize([(k,)+(v,) for k,v in traind_clusters.items()]).toDF(['cluster','center'])\n",
    "train_df_centers.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled_features</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.34782608...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.34782608...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.47826086...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.65217391...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13043478...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Scaled_features  cluster\n",
       "0  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.34782608...        6\n",
       "1  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.34782608...        6\n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.47826086...        7\n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.65217391...        7\n",
       "4  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13043478...        5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_df = prediction.withColumn('cluster',F.col('cluster').cast(IntegerType()))\n",
    "train_pred_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Joining of centroid and feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>Scaled_features</th>\n",
       "      <th>center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                    Scaled_features  \\\n",
       "0        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...   \n",
       "1        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...   \n",
       "2        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...   \n",
       "3        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...   \n",
       "4        0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43478260...   \n",
       "\n",
       "                                              center  \n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...  \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...  \n",
       "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...  \n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...  \n",
       "4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53799193...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_df = train_pred_df.join(train_df_centers,on='cluster',how='left')\n",
    "train_pred_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Anomaly Values\n",
    "#### Getting distance values function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dist = F.udf(lambda features, center :\n",
    "                 float(features.squared_distance(center)),FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the furthest distance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>Scaled_features</th>\n",
       "      <th>center</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11524</th>\n",
       "      <td>9</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.17391304...</td>\n",
       "      <td>[0.9997964169381108, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.018197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24323</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.43478260...</td>\n",
       "      <td>[0.0, 0.9998103186646434, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.007943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.78260869...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...</td>\n",
       "      <td>1.990329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7188</th>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.78260869...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...</td>\n",
       "      <td>1.989536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14083</th>\n",
       "      <td>5</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39130434...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.9995278564683664, 0.0, 0.0, ...</td>\n",
       "      <td>1.005527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>6</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56521739...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.9984000000000001, 0.0, ...</td>\n",
       "      <td>0.997304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...</td>\n",
       "      <td>0.986932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32873</th>\n",
       "      <td>4</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.920326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32872</th>\n",
       "      <td>4</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.830496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32871</th>\n",
       "      <td>4</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.745418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cluster                                    Scaled_features  \\\n",
       "11524        9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.17391304...   \n",
       "24323        3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.43478260...   \n",
       "7189         7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.78260869...   \n",
       "7188         7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.78260869...   \n",
       "14083        5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39130434...   \n",
       "8046         6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56521739...   \n",
       "7433         7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...   \n",
       "32873        4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "32872        4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "32871        4  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "\n",
       "                                                  center      dist  \n",
       "11524  [0.9997964169381108, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.018197  \n",
       "24323  [0.0, 0.9998103186646434, 0.0, 0.0, 0.0, 0.0, ...  2.007943  \n",
       "7189   [0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...  1.990329  \n",
       "7188   [0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...  1.989536  \n",
       "14083  [0.0, 0.0, 0.0, 0.9995278564683664, 0.0, 0.0, ...  1.005527  \n",
       "8046   [0.0, 0.0, 0.0, 0.0, 0.9984000000000001, 0.0, ...  0.997304  \n",
       "7433   [0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, ...  0.986932  \n",
       "32873  [0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.920326  \n",
       "32872  [0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.830496  \n",
       "32871  [0.9999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.745418  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_df = train_pred_df.withColumn('dist',get_dist(F.col('Scaled_features'),F.col('center')))\n",
    "train_pred_df.toPandas().sort_values(by=\"dist\",ascending=False).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- Fisrt method to detect Anomlay \n",
    "Calculate the number of outliers based on the given fraction and flag outliers:\n",
    "\n",
    "number_of_outliers = int(outliers_fraction * distances.count())\n",
    "\n",
    "distances.count()=Numbers of rows =38864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take 0.001 as fraction \n",
    "outliers_fraction=0.001\n",
    "distances_count=38864\n",
    "number_of_outliers = int(outliers_fraction * distances_count)\n",
    "outliers = train_pred_df.orderBy(train_pred_df[\"dist\"].desc()).take(number_of_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cluster=9, Scaled_features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1739, 0.0, 0.0]), center=[0.9997964169381108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00020358306188925082, 0.2728101543690696, 0.06701760508763766, 0.06884076411509232], dist=2.0181968212127686),\n",
       " Row(cluster=3, Scaled_features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4348, 0.0, 0.0104]), center=[0.0, 0.9998103186646434, 0.0, 0.0, 0.0, 0.0, 0.00018968133535660092, 0.3981328758989246, 0.05156622588337306, 0.07896868677288828], dist=2.0079429149627686),\n",
       " Row(cluster=7, Scaled_features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7826, 0.0476, 0.0104]), center=[0.0, 0.0, 0.0, 0.0, 0.0, 0.9929245283018867, 0.0047169811320754715, 0.6712469237079574, 0.011680143755615447, 0.02260220125786163], dist=1.9903289079666138)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Calculation of Threshold Value according to distance\n",
    "## Average distance for every cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>avgDist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.029433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.026675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.030295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster   avgDist\n",
       "0        1  0.026767\n",
       "1        6  0.029433\n",
       "2        3  0.026675\n",
       "3        5  0.030295\n",
       "4        9  0.023700"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageDistance = train_pred_df.groupBy(\"cluster\").agg(F.avg(\"dist\").alias(\"avgDist\"))\n",
    "averageDistance.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maximum distance for every cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>maxDist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.997304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.007943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.005527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2.018197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster   maxDist\n",
       "0        1  0.408354\n",
       "1        6  0.997304\n",
       "2        3  2.007943\n",
       "3        5  1.005527\n",
       "4        9  2.018197"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxDistance = train_pred_df.groupBy(\"cluster\")\\\n",
    ".agg(F.max(\"dist\").alias(\"maxDist\"))\n",
    "maxDistance.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining of predicted and threshold dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>Scaled_features</th>\n",
       "      <th>center</th>\n",
       "      <th>dist</th>\n",
       "      <th>cluster</th>\n",
       "      <th>maxDist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69565217...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                    Scaled_features  \\\n",
       "0        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...   \n",
       "1        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69565217...   \n",
       "2        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "3        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "4        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "\n",
       "                                              center      dist  cluster  \\\n",
       "0  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.032342        1   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.019759        1   \n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.011174        1   \n",
       "3  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.006572        1   \n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.006723        1   \n",
       "\n",
       "    maxDist  \n",
       "0  0.408354  \n",
       "1  0.408354  \n",
       "2  0.408354  \n",
       "3  0.408354  \n",
       "4  0.408354  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalyDetection = train_pred_df.join(maxDistance, maxDistance.cluster == train_pred_df.cluster)\n",
    "anomalyDetection.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>Scaled_features</th>\n",
       "      <th>center</th>\n",
       "      <th>dist</th>\n",
       "      <th>cluster</th>\n",
       "      <th>maxDist</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69565217...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                    Scaled_features  \\\n",
       "0        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65217391...   \n",
       "1        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69565217...   \n",
       "2        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "3        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "4        1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73913043...   \n",
       "\n",
       "                                              center      dist  cluster  \\\n",
       "0  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.032342        1   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.019759        1   \n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.011174        1   \n",
       "3  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.006572        1   \n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79982313...  0.006723        1   \n",
       "\n",
       "    maxDist detected  \n",
       "0  0.408354   Normal  \n",
       "1  0.408354   Normal  \n",
       "2  0.408354   Normal  \n",
       "3  0.408354   Normal  \n",
       "4  0.408354   Normal  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning of labels as normal or anomaly\n",
    "detected_df = anomalyDetection.withColumn(\"detected\", F.when((anomalyDetection.dist > (anomalyDetection.maxDist*0.99)), \"Anomaly\").otherwise(\"Normal\"))\n",
    "detected_df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerLocationState: string (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "# set microbatch interval as 10 seconds, this can be customized according to the project\n",
    "ssc = StreamingContext(sc,10)\n",
    "schema = StructType(\n",
    "[\n",
    "    StructField(\"customerLocationState\", StringType(), True),\n",
    "    StructField(\"startTime\",  TimestampType(), True),\n",
    "    StructField(\"endTime\",  TimestampType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True)\n",
    "])\n",
    "locations = spark.readStream \\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".schema(schema)\\\n",
    ".load(\"C:/Users/rzouga/Downloads/Work Swiss/location-exploration/Recruitment-Challenge/locations*.csv\")\n",
    "\n",
    "locations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[customerLocationState: string, startTime: timestamp, endTime: timestamp, duration: double, processingTime: timestamp, hourstart: int, date: date]>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "w = F.window('startTime', '3600 seconds')\n",
    "locations = locations.withColumn(\"processingTime\",F.current_timestamp())\n",
    "locations =locations.withColumn(\"hourstart\",F.hour(F.col(\"startTime\")))\n",
    "locations=locations.withColumn(\"date\", F.to_date(F.col(\"startTime\")))\n",
    "locations.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "countByCustomerPerday = locations\\\n",
    ".selectExpr(\n",
    "\"customerLocationState\",\"duration\",\n",
    "\"date\" ,\n",
    "\"hourstart\",\"startTime\" )\\\n",
    ".withWatermark(\"startTime\", \"1 hours\")\\\n",
    ".groupBy(\"date\",\"customerLocationState\").count().alias(\"counts1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "countByCustomerPerHour = locations\\\n",
    ".selectExpr(\n",
    "\"customerLocationState\",\"duration\",\n",
    " \"startTime\",\n",
    "\"date\" ,\n",
    "\"hourstart\" )\\\n",
    ".withWatermark(\"startTime\", \"1 hours\")\\\n",
    ".groupBy(\"date\",\"customerLocationState\",\"hourstart\").count().alias(\"counts2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countByCustomerPerHour.isStreaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= countByCustomerPerHour.writeStream\\\n",
    "  .format('console')\\.queryName(\"location_counts\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"select * from location_counts\").show(3)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationsStream = get_dummy(locations,catcols,num_cols)\n",
    "DataStream= scalerModel.transform(locationsStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Process\n",
    "## Prediction of Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_prediction = model.transform(DataStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_pred = streaming_prediction.withColumn('prediction',F.col('prediction').cast(IntegerType()))\n",
    "\n",
    "df_pred = df_pred.join(train_df_centers,on='prediction',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Euclidean Distance for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dist = funcs.udf(lambda features, center : \n",
    "                 float(features.squared_distance(center)),FloatType())\n",
    "\n",
    "df_pred = df_pred.withColumn('dist',get_dist(funcs.col('features'),funcs.col('center')))\n",
    "df_pred = df_pred.withColumnRenamed(\"prediction\", \"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Average and Maximum Distance for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = maxDist_streaming.withColumn(\"detected\", funcs.when(maxDist_streaming.dist > maxDist_streaming.maxDist, \"Anomaly\").otherwise(\"Normal\"))\n",
    "detection = detection.select( \"prediction\", \"dist\", \"maxDist\", \"status\", \"detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Sliding Window Time using Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentTimeDf = detection.withColumn(\"processingTime2\",funcs.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCount = currentTimeDf\\\n",
    ".withWatermark(\"processingTime\", \"5 seconds\")\\\n",
    ".groupBy(F.window(\"processingTime\", \"4 seconds\", \"2 seconds\"), \"prediction\", \"detected\", \"dist\", \"maxDist\")\\\n",
    ".avg(\"dist\").orderBy(\"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Streaming\n",
    "## Option 1 - Using Sliding Window and Watermarking (Confusion Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = detection.groupBy(\"detected\").count().select( \"detected\",\"count\")\n",
    "confusion_matrix = currentTimeDf\\\n",
    ".withWatermark(\"processingTime\", \"5 seconds\")\\\n",
    ".groupBy(funcs.window(\"processingTime\", \"3 seconds\", \"1 seconds\"),\"status\", \"detected\")\\\n",
    ".count().orderBy(\"window\")\n",
    "q = confusion_matrix.writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".format(\"console\")\\\n",
    ".option(\"truncate\", \"false\")\\\n",
    ".start()\n",
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 - Using Append method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = windowedCount.writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".format(\"console\")\\\n",
    ".option(\"truncate\", \"false\")\\\n",
    ".start()\n",
    "\n",
    "\n",
    "q.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3 - Using Complete and Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df_pred.groupBy([\"cluster\",\"status\"]).count()\\\n",
    ".orderBy(\"cluster\", \"status\", ascending=True)\n",
    "\n",
    "q = group.writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".format(\"console\")\\\n",
    ".start()\n",
    "\n",
    "\n",
    "\n",
    "q.awaitTermination()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
